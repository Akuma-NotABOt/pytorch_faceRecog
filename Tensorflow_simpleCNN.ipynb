{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8e8497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "detector = MTCNN()\n",
    "def cropFaces(img):\n",
    "    \n",
    "    # detect faces in the image\n",
    "    faces = detector.detect_faces(img)\n",
    "    #print(faces)\n",
    "    if len(faces) == 0:\n",
    "        return faces\n",
    "    x, y, width, height = faces[0]['box']\n",
    "    #img[y:y+height , x:x+width, :]\n",
    "    return img[y:y+height, x:x+width, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c7cf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 images belonging to 5 classes.\n",
      "Found 75 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set the paths to your training and test data directories\n",
    "# The images should be in seperate folders labelled with their corresponding names\n",
    "train_data_dir = r\"C:\\Users\\jv258\\Documents\\Jupyter\\Projects\\Jodhpur\\Images\\Train\" #Edit path to Training image\n",
    "test_data_dir = r\"C:\\Users\\jv258\\Documents\\Jupyter\\Projects\\Jodhpur\\Images\\Test\" #Edit path to Test images\n",
    "\n",
    "# Define the image dimensions and batch size\n",
    "img_width, img_height = 150, 150\n",
    "batch_size = 32\n",
    "\n",
    "# Preprocess, Augment and Normalising the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_width, img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "# Preprocess the test data (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                                  target_size=(img_width, img_height),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74d0799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (32, 150, 150, 3)\n",
      "Labels: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_generator:\n",
    "    print(\"Batch shape:\", images.shape)\n",
    "    print(\"Labels:\", labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97139b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes\n",
    "num_classes = int(labels[0].size)\n",
    "\n",
    "# Create the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7282d85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 7s 2s/step - loss: 1.8257 - accuracy: 0.1600 - val_loss: 1.6230 - val_accuracy: 0.2000\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 3s 816ms/step - loss: 1.6235 - accuracy: 0.2000 - val_loss: 1.6116 - val_accuracy: 0.2267\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 2s 659ms/step - loss: 1.5947 - accuracy: 0.2800 - val_loss: 1.6118 - val_accuracy: 0.2000\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.6274 - accuracy: 0.1867 - val_loss: 1.6252 - val_accuracy: 0.2000\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 3s 830ms/step - loss: 1.5997 - accuracy: 0.2533 - val_loss: 1.5924 - val_accuracy: 0.2267\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.5709 - accuracy: 0.3467 - val_loss: 1.5789 - val_accuracy: 0.2133\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 2s 973ms/step - loss: 1.5880 - accuracy: 0.2267 - val_loss: 1.5893 - val_accuracy: 0.2667\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 3s 773ms/step - loss: 1.5495 - accuracy: 0.3867 - val_loss: 1.5537 - val_accuracy: 0.2800\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.4782 - accuracy: 0.4000 - val_loss: 1.5372 - val_accuracy: 0.3600\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 3s 976ms/step - loss: 1.5302 - accuracy: 0.3467 - val_loss: 1.5286 - val_accuracy: 0.3067\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 3s 917ms/step - loss: 1.5040 - accuracy: 0.3600 - val_loss: 1.5187 - val_accuracy: 0.2933\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 3s 919ms/step - loss: 1.4222 - accuracy: 0.3733 - val_loss: 1.5460 - val_accuracy: 0.2800\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 3s 844ms/step - loss: 1.3490 - accuracy: 0.3333 - val_loss: 1.4964 - val_accuracy: 0.3600\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 2s 821ms/step - loss: 1.4764 - accuracy: 0.3867 - val_loss: 1.5115 - val_accuracy: 0.3200\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 3s 879ms/step - loss: 1.2544 - accuracy: 0.4533 - val_loss: 1.5637 - val_accuracy: 0.3467\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.4310 - accuracy: 0.5333 - val_loss: 1.4338 - val_accuracy: 0.3600\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 3s 858ms/step - loss: 1.2814 - accuracy: 0.4533 - val_loss: 1.3830 - val_accuracy: 0.3867\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.2584 - accuracy: 0.5467 - val_loss: 1.4138 - val_accuracy: 0.3733\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.2339 - accuracy: 0.5067 - val_loss: 1.3534 - val_accuracy: 0.4400\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 2s 816ms/step - loss: 1.1290 - accuracy: 0.5333 - val_loss: 1.4150 - val_accuracy: 0.4133\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.1176 - accuracy: 0.5600 - val_loss: 1.3200 - val_accuracy: 0.4933\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.1355 - accuracy: 0.5333 - val_loss: 1.3279 - val_accuracy: 0.4800\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 3s 861ms/step - loss: 1.0702 - accuracy: 0.5467 - val_loss: 1.2890 - val_accuracy: 0.4667\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 3s 996ms/step - loss: 1.1195 - accuracy: 0.5200 - val_loss: 1.3969 - val_accuracy: 0.4267\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.0355 - accuracy: 0.6267 - val_loss: 1.4153 - val_accuracy: 0.4667\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 3s 903ms/step - loss: 0.9862 - accuracy: 0.6800 - val_loss: 1.5577 - val_accuracy: 0.4800\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 2s 700ms/step - loss: 1.0744 - accuracy: 0.5333 - val_loss: 1.3641 - val_accuracy: 0.5067\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 3s 863ms/step - loss: 1.1008 - accuracy: 0.5867 - val_loss: 1.3562 - val_accuracy: 0.5067\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 3s 949ms/step - loss: 1.0033 - accuracy: 0.6133 - val_loss: 1.4174 - val_accuracy: 0.5067\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.1280 - accuracy: 0.5333 - val_loss: 1.3312 - val_accuracy: 0.5067\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.1338 - accuracy: 0.5467 - val_loss: 1.3924 - val_accuracy: 0.4267\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 3s 973ms/step - loss: 1.1479 - accuracy: 0.5067 - val_loss: 1.3479 - val_accuracy: 0.5200\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 3s 694ms/step - loss: 0.9679 - accuracy: 0.5733 - val_loss: 1.4630 - val_accuracy: 0.4667\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 3s 848ms/step - loss: 1.0320 - accuracy: 0.5600 - val_loss: 1.2678 - val_accuracy: 0.4800\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 2s 871ms/step - loss: 0.8875 - accuracy: 0.6533 - val_loss: 1.4134 - val_accuracy: 0.4133\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.8866 - accuracy: 0.6667 - val_loss: 1.3283 - val_accuracy: 0.4933\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 3s 951ms/step - loss: 0.9357 - accuracy: 0.6400 - val_loss: 1.4294 - val_accuracy: 0.4933\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 3s 797ms/step - loss: 0.9945 - accuracy: 0.6133 - val_loss: 1.3252 - val_accuracy: 0.5067\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 3s 926ms/step - loss: 0.9009 - accuracy: 0.6133 - val_loss: 1.3886 - val_accuracy: 0.4667\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 3s 998ms/step - loss: 0.9103 - accuracy: 0.6000 - val_loss: 1.4647 - val_accuracy: 0.4533\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 2s 769ms/step - loss: 0.7877 - accuracy: 0.6800 - val_loss: 1.3972 - val_accuracy: 0.4533\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 2s 581ms/step - loss: 0.9475 - accuracy: 0.6133 - val_loss: 1.3974 - val_accuracy: 0.4933\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 3s 853ms/step - loss: 0.8393 - accuracy: 0.6533 - val_loss: 1.5251 - val_accuracy: 0.4533\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 3s 785ms/step - loss: 0.8523 - accuracy: 0.6400 - val_loss: 1.3432 - val_accuracy: 0.4533\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 3s 989ms/step - loss: 0.8002 - accuracy: 0.6800 - val_loss: 1.5950 - val_accuracy: 0.4800\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 3s 744ms/step - loss: 0.8128 - accuracy: 0.6267 - val_loss: 1.5066 - val_accuracy: 0.5467\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.9432 - accuracy: 0.6267 - val_loss: 1.4603 - val_accuracy: 0.4800\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 2s 643ms/step - loss: 0.8827 - accuracy: 0.6267 - val_loss: 1.4562 - val_accuracy: 0.4667\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.7447 - accuracy: 0.7200 - val_loss: 1.6767 - val_accuracy: 0.4933\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 3s 957ms/step - loss: 0.7450 - accuracy: 0.7200 - val_loss: 1.5344 - val_accuracy: 0.4800\n"
     ]
    }
   ],
   "source": [
    "#the model is initialized to use the adam algorithm\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# It was noticed that the number of epochs had an effect on the model accuracy. But since \n",
    "# the accuracy did not show much deviation and reached a maximum after 50 epochs the epochs is set to 60\n",
    "model.fit(train_generator,\n",
    "          steps_per_epoch=len(train_generator),\n",
    "          epochs=50,\n",
    "          validation_data=test_generator,\n",
    "          validation_steps=len(test_generator))\n",
    "\n",
    "model.save('image_recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b2d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the full path of the image without\"\":\n",
      "f\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_recognition_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter the full path of the image without\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_height\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m test_image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimg_to_array(test_image)\n\u001b[0;32m      7\u001b[0m test_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\image_utils.py:422\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    423\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'f'"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('image_recognition_model.h5')\n",
    "\n",
    "test_image_path = input('Enter the full path of the image without\"\":\\n')\n",
    "\n",
    "test_image = tf.keras.preprocessing.image.load_img(test_image_path, target_size=(img_width, img_height))\n",
    "test_image = tf.keras.preprocessing.image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "test_image = test_image / 255.0\n",
    "\n",
    "predictions = loaded_model.predict(test_image)\n",
    "predicted_label = np.argmax(predictions)\n",
    "\n",
    "class_labels = train_generator.class_indices\n",
    "person_label = {v: k for k, v in class_labels.items()}\n",
    "predicted_person = person_label[predicted_label]\n",
    "\n",
    "print('Predicted Person:', predicted_person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71331c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model has some inaccuracies which can be due to the algorithm\n",
    "# or the limited number of training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb63d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "Predicted Person: Zac Efron\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted Person: Keira Knightley\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted Person: Tom Cruise\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted Person: Shahrukh Khan\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Person: Keira Knightley\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted Person: Barack Obama\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Person: Keira Knightley\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "Predicted Person: Tom Cruise\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted Person: Shahrukh Khan\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Predicted Person: Zac Efron\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted Person: Keira Knightley\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Person: Zac Efron\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted Person: Keira Knightley\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted Person: Shahrukh Khan\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Predicted Person: Tom Cruise\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Set the path to the folder containing the images\n",
    "folder_path = r\"C:\\Users\\jv258\\Documents\\Jupyter\\Projects\\Jodhpur\\Images\\Test\\Keira Knightley\"\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "        \n",
    "    image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        \n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    image = cv2.resize(image, (150, 150))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)         \n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image / 255.0\n",
    "    \n",
    "    pred = loaded_model.predict(image)\n",
    "    \n",
    "    pred_label = np.argmax(pred)\n",
    "    class_labels = train_generator.class_indices\n",
    "    person_label = {v: k for k, v in class_labels.items()}\n",
    "    pred_person = person_label[pred_label]\n",
    "        \n",
    "    print('Predicted Person:', pred_person)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21462822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
